{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNAvdA5x9YEKd+WSPfBynX5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fatemafaria142/Exploration-of-Different-Prompting-Techniques-for-Automatic-Realistic-Story-Generation/blob/main/story_generation_zero_shot_prompt_using_GPT2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6LzHgS_Fkh5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "mNz5iKXhFug-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers"
      ],
      "metadata": {
        "id": "ENBd58R5Ft_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[torch]"
      ],
      "metadata": {
        "id": "gXFW8F5HFuEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate -U"
      ],
      "metadata": {
        "id": "HZyi2-_mFuJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Dataset Link:** https://huggingface.co/datasets/AtlasUnified/atlas-storyteller?row=42"
      ],
      "metadata": {
        "id": "hka_zLJTff6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"AtlasUnified/atlas-storyteller\")"
      ],
      "metadata": {
        "id": "5FVft-5zF_f0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)"
      ],
      "metadata": {
        "id": "5fpMKflMF_i1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "nFm7F9YMGFe-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Get the first 5000 data points**"
      ],
      "metadata": {
        "id": "m2wHOnhdfs_h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the first 5000 data points\n",
        "num_samples_to_display = 5000\n",
        "subset_dataset = dataset['train'].select(range(num_samples_to_display))\n",
        "\n",
        "# Display information for 3 data points from the subset\n",
        "num_samples_to_show = 3\n",
        "for i in range(num_samples_to_show):\n",
        "    data = subset_dataset[i]\n",
        "    print(f\"Data Point {i + 1}:\")\n",
        "    print(\"ID:\", data['id'])\n",
        "    print(\"Input:\", data['Story'])\n",
        "    print(\"\\n-----------------------------\\n\")"
      ],
      "metadata": {
        "id": "m5omug3zGRnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Zero-Shot Story Generation Prompt**"
      ],
      "metadata": {
        "id": "4T31f3CMfuzc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_zero_shot_prompt(x):\n",
        "    result = f\"### Instruction:\\nGenerate a story based on the following prompt:\\n\\n{x['Story']}\"\n",
        "    return result"
      ],
      "metadata": {
        "id": "5gC0JXSyHaVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Display prompts for the first 5 data points**"
      ],
      "metadata": {
        "id": "570EBoeugECb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate prompts for each data point in the subset dataset\n",
        "prompts = []\n",
        "for i in range(num_samples_to_display):\n",
        "    data = subset_dataset[i]\n",
        "    prompt = get_zero_shot_prompt(data)\n",
        "    prompts.append(prompt)\n",
        "\n",
        "# Display the generated prompts or use them as needed\n",
        "for idx, prompt in enumerate(prompts[:5]):  # Display prompts for the first 5 data points\n",
        "    print(f\"Prompt for Data Point {idx + 1}:\")\n",
        "    print(prompt)\n",
        "    print(\"\\n-----------------------------\\n\")\n"
      ],
      "metadata": {
        "id": "8deQRMNlJcAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **More Examples of Prompts**"
      ],
      "metadata": {
        "id": "wIV0IPUNgIrU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate prompts for each data point in the subset dataset\n",
        "prompts = []\n",
        "for i in range(num_samples_to_display):\n",
        "    data = subset_dataset[i]\n",
        "    prompt = get_zero_shot_prompt(data)\n",
        "    prompts.append(prompt)\n",
        "\n",
        "# Display the generated prompts or use them as needed\n",
        "for idx, prompt in enumerate(prompts[5:10]):  # Display prompts for the first 3 data points\n",
        "    print(f\"Prompt for Data Point {idx + 1}:\")\n",
        "    print(prompt)\n",
        "    print(\"\\n-----------------------------\\n\")\n"
      ],
      "metadata": {
        "id": "jIA4PQZEPjxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GPT2 and its tokenizer**\n",
        "* https://huggingface.co/docs/transformers/model_doc/gpt2"
      ],
      "metadata": {
        "id": "ARL-NrpAgSSZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
        "model = AutoModelForCausalLM.from_pretrained('gpt2')"
      ],
      "metadata": {
        "id": "LaRsH5I-MbGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the padding token for the tokenizer\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "sqGEsD81CslS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dataset Preparation and Tokenization for GPT-2 Training**\n",
        "* This code segment appears to perform the following tasks:\n",
        "\n",
        "* Assumes a dataset with specific keys ('instruction', 'input', 'output', and 'text').\n",
        "* Iterates through the dataset to generate prompts based on the available data points.\n",
        "* Tokenizes the generated prompts and output text using a tokenizer, preparing them as tensors.\n",
        "* Compiles tokenized inputs, labels, and attention masks to be used for GPT-2 model training."
      ],
      "metadata": {
        "id": "YQ_YQmJZg1GH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "import torch\n",
        "# Assuming subset_dataset is your dataset with 'id', 'Story'\n",
        "dataset = subset_dataset\n",
        "\n",
        "# Initialize empty lists to store inputs, targets, and attention masks\n",
        "input_ids = []\n",
        "labels = []\n",
        "attention_masks = []\n",
        "\n",
        "# Tokenize and prepare the dataset\n",
        "for data_point in dataset:\n",
        "    # Generate prompt\n",
        "    prompt = get_zero_shot_prompt(data_point)\n",
        "\n",
        "    # Tokenize prompt\n",
        "    tokenized_prompts = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "    # Tokenize output text (corrected text)\n",
        "    tokenized_output = tokenizer(data_point['Story'], return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "    # Append tokenized inputs, labels, and attention masks\n",
        "    input_ids.append(tokenized_prompts['input_ids'])\n",
        "    labels.append(tokenized_output['input_ids'])\n",
        "    attention_masks.append(tokenized_prompts['attention_mask'])\n",
        "\n",
        "# Convert lists to tensors\n",
        "input_ids = torch.stack(input_ids)\n",
        "labels = torch.stack(labels)\n",
        "attention_masks = torch.stack(attention_masks)"
      ],
      "metadata": {
        "id": "sZ3qkj3RQRix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Printing input_ids, labels, and attention_masks for the first 5 examples**"
      ],
      "metadata": {
        "id": "fV2lLzKngnOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming the code provided earlier to prepare the dataset is already executed\n",
        "\n",
        "# Printing input_ids, labels, and attention_masks for the first 5 examples\n",
        "for i in range(3):\n",
        "    print(f\"Example {i+1}:\")\n",
        "    print(\"Input IDs:\", input_ids[i])\n",
        "    print(\"Labels:\", labels[i])\n",
        "    print(\"Attention Mask:\", attention_masks[i])\n",
        "    print(\"-----------------------\")\n"
      ],
      "metadata": {
        "id": "K7LmtvNYN2Da"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dynamic Data Collation for GPT-2 Model Training**\n",
        "* This code segment encompasses a class, GPT2DataCollator, which is designed to handle the collation of input features for GPT-2 model training. It dynamically checks the type of input features (whether they are dictionaries or tuples) and appropriately extracts input IDs, attention masks, and labels for padding and preparing the data to the same length. This data collation process is crucial for ensuring the uniformity and compatibility of the input features during the training of the GPT-2 model."
      ],
      "metadata": {
        "id": "VH8dh2xhhAET"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "import torch\n",
        "\n",
        "\n",
        "class GPT2DataCollator:\n",
        "    def __call__(self, features):\n",
        "        # Check if the features are dictionaries or tuples\n",
        "        if isinstance(features[0], dict):\n",
        "            input_ids = [feature['input_ids'] for feature in features]\n",
        "            attention_masks = [feature['attention_mask'] for feature in features]\n",
        "            labels = [feature['labels'] for feature in features]\n",
        "        else:  # Assuming features are tuples\n",
        "            input_ids = [feature[0] for feature in features]\n",
        "            attention_masks = [feature[1] for feature in features]\n",
        "            labels = [feature[2] for feature in features]\n",
        "\n",
        "        # Pad inputs and labels to the same length\n",
        "        input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "        attention_masks = torch.nn.utils.rnn.pad_sequence(attention_masks, batch_first=True, padding_value=0)\n",
        "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_masks,\n",
        "            'labels': labels\n",
        "        }\n"
      ],
      "metadata": {
        "id": "iYwb-UutTO1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Define the Training Arguments and Trainer**"
      ],
      "metadata": {
        "id": "uSD-pX5nhYmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# Define the training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./gpt2-finetuned-version',    # Directory to save the model and checkpoints\n",
        "    num_train_epochs=5,               # Number of training epochs\n",
        "    per_device_train_batch_size=4,    # Batch size per device during training\n",
        "    save_steps=5000,                  # Save checkpoint every X steps\n",
        "    logging_dir='./logs',             # Directory for storing logs\n",
        "    logging_steps=500,                # Log training metrics every X steps\n",
        "    evaluation_strategy=\"epoch\",      # Evaluation strategy to adopt during training\n",
        "    report_to=\"none\",                 # Disable evaluation during training\n",
        "    prediction_loss_only=True,        # Compute only the prediction loss\n",
        "    warmup_steps=500# number of warmup steps for learning rate scheduler\n",
        "    # Add any additional arguments as needed\n",
        ")\n",
        "\n",
        "# Initialize the Trainer with the custom data collator\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=torch.utils.data.TensorDataset(input_ids, attention_masks, labels),\n",
        "    eval_dataset=torch.utils.data.TensorDataset(input_ids, attention_masks, labels),\n",
        "    data_collator=GPT2DataCollator()  # Use the custom data collator\n",
        ")\n",
        "'''"
      ],
      "metadata": {
        "id": "RHMSmJaATStX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# Start training\n",
        "trainer.train()\n",
        "'''"
      ],
      "metadata": {
        "id": "JLjHNwryDOJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "trainer.save_model()\n",
        "'''"
      ],
      "metadata": {
        "id": "RzqCF7I6DOTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load the save model and train it again here...**"
      ],
      "metadata": {
        "id": "pi7ONgjQhifB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "# Load the previously trained model\n",
        "model_path = './gpt2-finetuned-version'  # Replace this with the path to your saved model\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "\n",
        "# Define the training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./gpt2-final-version',    # Directory to save the model and checkpoints\n",
        "    num_train_epochs=5,               # Number of training epochs\n",
        "    per_device_train_batch_size=4,    # Batch size per device during training\n",
        "    save_steps=1000,                  # Save checkpoint every X steps\n",
        "    logging_dir='./logs',             # Directory for storing logs\n",
        "    logging_steps=500,                # Log training metrics every X steps\n",
        "    evaluation_strategy=\"epoch\",      # Evaluation strategy to adopt during training\n",
        "    report_to=\"none\",                 # Disable evaluation during training\n",
        "    prediction_loss_only=True,        # Compute only the prediction loss\n",
        "    warmup_steps=500# number of warmup steps for learning rate scheduler\n",
        "    # Add any additional arguments as needed\n",
        ")\n",
        "\n",
        "# Initialize the Trainer with the custom data collator\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=torch.utils.data.TensorDataset(input_ids, attention_masks, labels),\n",
        "    eval_dataset=torch.utils.data.TensorDataset(input_ids, attention_masks, labels),\n",
        "    data_collator=GPT2DataCollator()  # Use the custom data collator\n",
        ")"
      ],
      "metadata": {
        "id": "IX4xjo1OTUTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start training\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "DFK04cXTUG_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save the fine-tuned model here\n",
        "trainer.save_model()"
      ],
      "metadata": {
        "id": "vYtiotAQaTNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate perplexity on the test dataset\n",
        "eval_result = trainer.evaluate(eval_dataset=torch.utils.data.TensorDataset(input_ids, attention_masks, labels))\n",
        "print(\"Perplexity:\", eval_result['eval_loss'])"
      ],
      "metadata": {
        "id": "nvjjPxfAjU7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Text Generation Using Fine-Tuned GPT-2 Model Pipeline**"
      ],
      "metadata": {
        "id": "-rcFwCmQhurf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Define the generator pipeline using the fine-tuned GPT-2 model and tokenizer\n",
        "generator = pipeline('text-generation', model='./gpt2-final-version', tokenizer='gpt2', pad_token_id=tokenizer.pad_token_id)\n",
        "\n",
        "# Test prompt for generating text\n",
        "test_prompt = (\n",
        "    f\"### Instruction:\\nGenerate a story based on the following prompt:\\n\\nAmidst the inky blackness of the night \"\n",
        ")\n",
        "\n",
        "# Generate text based on the test prompt using the generator pipeline\n",
        "generated_test_text = generator(test_prompt, max_length=128, num_return_sequences=1)\n",
        "\n",
        "# Print the generated text for the test prompt\n",
        "print(generated_test_text[0]['generated_text'])\n",
        "\n"
      ],
      "metadata": {
        "id": "BVC4UIKUYw7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **More Text Generation Examples**"
      ],
      "metadata": {
        "id": "2xegPLZi1cwY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"AtlasUnified/atlas-storyteller\")"
      ],
      "metadata": {
        "id": "cSY8ypFD0_O9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the first 10 data points\n",
        "num_samples_to_display = 10\n",
        "subset_dataset = dataset['train'].select(range(num_samples_to_display))\n",
        "\n",
        "# List to store 'Story' values\n",
        "story_list = []\n",
        "\n",
        "# Display information for 10 data points from the subset and save 'Story' values\n",
        "num_samples_to_show = 10\n",
        "for i in range(num_samples_to_show):\n",
        "    data = subset_dataset[i]\n",
        "\n",
        "    # Save 'Story' into the list\n",
        "    story_list.append(data['Story'])\n",
        "\n",
        "    print(f\"Data Point {i + 1}:\")\n",
        "    print(\"ID:\", data['id'])\n",
        "    print(\"Story:\", data['Story'])\n",
        "    print(\"\\n-----------------------------\\n\")"
      ],
      "metadata": {
        "id": "azJOi1Pd1E3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "\n",
        "# Define the generator pipeline using the fine-tuned GPT-2 model and tokenizer\n",
        "generator = pipeline('text-generation', model='./gpt2-final-version', tokenizer='gpt2', pad_token_id=tokenizer.pad_token_id)\n",
        "\n",
        "# List to store generated stories\n",
        "generated_stories = []\n",
        "\n",
        "# Generate text based on the subset of the training dataset\n",
        "for example in subset_dataset:\n",
        "    # Extract the first 7 words from example['Story']\n",
        "    first_7_words = \" \".join(example['Story'].split()[:7])\n",
        "\n",
        "    # Create the test prompt using the extracted words\n",
        "    test_prompt = (\n",
        "        f\"### Instruction:\\nGenerate a story based on the following prompt:\\n\\n{first_7_words} \"\n",
        "    )\n",
        "\n",
        "    # Generate text based on the test prompt using the generator pipeline\n",
        "    generated_text = generator(test_prompt, max_length=128, num_return_sequences=1)\n",
        "\n",
        "    # Extract and store only the generated story\n",
        "    generated_story = generated_text[0]['generated_text'].split(\"### Instruction:\\nGenerate a story based on the following prompt:\\n\\n\")[1].strip()\n",
        "    generated_stories.append(generated_story)\n",
        "\n",
        "# Print or use the list of generated stories\n",
        "for idx, story in enumerate(generated_stories):\n",
        "    print(f\"Generated Story {idx + 1}:\\n{story}\")\n",
        "    print(\"------------------------------------------\")\n"
      ],
      "metadata": {
        "id": "uoXcxIUw3pmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BERTScore**\n",
        "* https://huggingface.co/spaces/evaluate-metric/bertscore"
      ],
      "metadata": {
        "id": "9a110rNUN-Hq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate"
      ],
      "metadata": {
        "id": "HVj06YmpWdQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bert_score"
      ],
      "metadata": {
        "id": "h11J_xJjWnmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sacrebleu"
      ],
      "metadata": {
        "id": "QnAgvC9Eam6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(story_list)"
      ],
      "metadata": {
        "id": "MkGgt3ghXezZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generated_stories)"
      ],
      "metadata": {
        "id": "yE_dsvJOXhhx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Partial match with the distilbert-base-uncased model:**"
      ],
      "metadata": {
        "id": "QN6ElEnxZicl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from evaluate import load\n",
        "from bert_score import score as bert_score\n",
        "from sacrebleu import corpus_bleu\n",
        "\n",
        "# Load BERTScore model\n",
        "bertscore = load(\"bertscore\")\n",
        "\n",
        "# Prepare data\n",
        "predictions = generated_stories\n",
        "references = story_list\n",
        "\n",
        "# Calculate BERTScore\n",
        "bert_results = bertscore.compute(predictions=predictions, references=story_list, model_type=\"distilbert-base-uncased\")\n",
        "\n",
        "# Print BERTScore results\n",
        "print(\"BERTScore Results:\")\n",
        "print(bert_results)\n",
        "\n",
        "# Load BLEU metrics\n",
        "bleu_metric = load(\"bleu\")\n",
        "\n",
        "# Calculate Bilingual Evaluation Understudy (BLEU)\n",
        "results_bleu = bleu_metric.compute(predictions=generated_stories, references=story_list)\n",
        "print(f\"\\nBLEU Score: {results_bleu }\\n\")"
      ],
      "metadata": {
        "id": "4UdvIJw3aTSz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}